{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ec513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45e179",
   "metadata": {},
   "source": [
    "## What is RAG (retrieval augmented generation)?\n",
    "Basically, shoving lot of extra information in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52bc9487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Ajit is Preeti's brother. He is also the brother of Sweta. This is stated in the context where it mentions Ajit has two sisters, Preeti and Sweta, implying Ajit is their male sibling.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 119, 'total_tokens': 168, 'completion_time': 0.172710254, 'prompt_time': 0.016677427, 'queue_time': 0.052131503, 'total_time': 0.189387681}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--2f526dde-3684-4c25-b446-cab089bae9c2-0' usage_metadata={'input_tokens': 119, 'output_tokens': 49, 'total_tokens': 168}\n"
     ]
    }
   ],
   "source": [
    "# Example \n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    model_provider=\"groq\",\n",
    "    api_key=\"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\")\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(\n",
    "    prompt_template.format(\n",
    "        context=\"Ajit has two sisters, Preeti and Sweta. Ajit is male.\",\n",
    "        question=\"Who is Preeti's bother?\"         \n",
    "    ))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475aef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajit is Preeti's brother. He is also the brother of Sweta. This is stated in the context where it mentions Ajit has two sisters, Preeti and Sweta, implying Ajit is their male sibling.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33703cf0",
   "metadata": {},
   "source": [
    ">RAG is all about cleverly pushing is as much information in the context with minimum possible tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# little non-trival RAG example\n",
    "\n",
    "# grab a novel\n",
    "file = open(\"The Old Curiosity Shop.txt\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07503397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Based on the provided text, here is the list of story titles with a short summary for each:\\n\\n1. **The Finest Story in the World** - This story is not provided in the text, so I'll skip it.\\n2. **With the Main Guard** - This story is not provided in the text, so I'll skip it.\\n3. **Wee Willie Winkie** - This story is not provided in the text, so I'll skip it.\\n4. **The Rout of the White Hussars** - This story is not provided in the text, so I'll skip it.\\n5. **At Twenty-two** - This story is not provided in the text, so I'll skip it.\\n6. **The Courting of Dinah Shadd** - This story is not provided in the text, so I'll skip it.\\n7. **The Story of Muhammad Din** - This story is not provided in the text, so I'll skip it.\\n\\nUnfortunately, none of the stories are provided in the text, so I couldn't write a summary for any of them. If you provide the actual stories, I'd be happy to help with the summaries. \\n\\nHowever, I can suggest that these stories are part of Rudyard Kipling's collection, and they might be related to his typical themes of colonialism, adventure, and human relationships, given his style and the time period in which he wrote.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 293, 'prompt_tokens': 309, 'total_tokens': 602, 'completion_time': 0.688740712, 'prompt_time': 0.030738753, 'queue_time': 0.047137196, 'total_time': 0.719479465}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_2ddfbb0da0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--f16a6295-f753-43dc-9d79-d2043eb65eeb-0' usage_metadata={'input_tokens': 309, 'output_tokens': 293, 'total_tokens': 602}\n"
     ]
    }
   ],
   "source": [
    "# lets try to talk with this book\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    model_provider=\"groq\",\n",
    "    api_key=\"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\")\n",
    "\n",
    "# Prepare your prompt\n",
    "prompt_template = \"\"\"You are a novel reader. You are given collection of stories:\n",
    "{collection_of_stories}\n",
    "You are tasked to make a list of story titles in this collection. Write a short summary for each story in English Language. Skip the story from the list, if the story is not provided in the text. \n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(collection_of_stories = text[:len(text)//2000]))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87249ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided text, here is the list of story titles with a short summary for each:\n",
       "\n",
       "1. **The Finest Story in the World** - This story is not provided in the text, so I'll skip it.\n",
       "2. **With the Main Guard** - This story is not provided in the text, so I'll skip it.\n",
       "3. **Wee Willie Winkie** - This story is not provided in the text, so I'll skip it.\n",
       "4. **The Rout of the White Hussars** - This story is not provided in the text, so I'll skip it.\n",
       "5. **At Twenty-two** - This story is not provided in the text, so I'll skip it.\n",
       "6. **The Courting of Dinah Shadd** - This story is not provided in the text, so I'll skip it.\n",
       "7. **The Story of Muhammad Din** - This story is not provided in the text, so I'll skip it.\n",
       "\n",
       "Unfortunately, none of the stories are provided in the text, so I couldn't write a summary for any of them. If you provide the actual stories, I'd be happy to help with the summaries. \n",
       "\n",
       "However, I can suggest that these stories are part of Rudyard Kipling's collection, and they might be related to his typical themes of colonialism, adventure, and human relationships, given his style and the time period in which he wrote."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better printing\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb7239b",
   "metadata": {},
   "source": [
    "### Problem: Too much raw information in the context makes the prompt too long.\n",
    "- Costly\n",
    "- adds noise\n",
    "### Solution: Use RAG\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "### To understand RAG, we need to understand Semantic Search\n",
    "https://python.langchain.com/docs/tutorials/retrievers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1ca6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text using RELEVANT loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"The Old curiosity Shop.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50eab39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split given book into 881 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# Split document into small chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split given book into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68dd2b5",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19bbad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=\"AIzaSyDQU_npYx8c_ntmKXQKBkU6GBmHV-ndHRI\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "758a909b",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:243\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[0;32m    244\u001b[0m         BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1437\u001b[0m, in \u001b[0;36mGenerativeServiceClient.batch_embed_contents\u001b[1;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1437\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1438\u001b[0m     request,\n\u001b[0;32m   1439\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1440\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1441\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1442\u001b[0m )\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    295\u001b[0m     target,\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    297\u001b[0m     sleep_generator,\n\u001b[0;32m    298\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    299\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    300\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[0;32m    157\u001b[0m         exc,\n\u001b[0;32m    158\u001b[0m         deadline,\n\u001b[0;32m    159\u001b[0m         sleep_iter,\n\u001b[0;32m    160\u001b[0m         error_list,\n\u001b[0;32m    161\u001b[0m         predicate,\n\u001b[0;32m    162\u001b[0m         on_error,\n\u001b[0;32m    163\u001b[0m         exception_factory,\n\u001b[0;32m    164\u001b[0m         timeout,\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m InMemoryVectorStore(embeddings)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Adding documents to vector store\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m document_ids \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mall_splits)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\in_memory.py:195\u001b[0m, in \u001b[0;36mInMemoryVectorStore.add_documents\u001b[1;34m(self, documents, ids, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add documents to the store.\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 195\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[0;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids must be the same length as texts. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m texts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:247\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    243\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[0;32m    244\u001b[0m             BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39membeddings])\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=\"AIzaSyDQU_npYx8c_ntmKXQKBkU6GBmHV-ndHRI\"\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "# (CLASSROOM DISCUSSION: What are vector stores? What do we make them?)\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Adding documents to vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e746c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m document_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document_ids' is not defined"
     ]
    }
   ],
   "source": [
    "document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6363c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:281\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_query\u001b[1;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[0;32m    275\u001b[0m     request: EmbedContentRequest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    276\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m    277\u001b[0m         task_type\u001b[38;5;241m=\u001b[39mtask_type,\n\u001b[0;32m    278\u001b[0m         title\u001b[38;5;241m=\u001b[39mtitle,\n\u001b[0;32m    279\u001b[0m         output_dimensionality\u001b[38;5;241m=\u001b[39moutput_dimensionality,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 281\u001b[0m     result: EmbedContentResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membed_content(request)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1305\u001b[0m, in \u001b[0;36mGenerativeServiceClient.embed_content\u001b[1;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1305\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1306\u001b[0m     request,\n\u001b[0;32m   1307\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1308\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1309\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1310\u001b[0m )\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    295\u001b[0m     target,\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    297\u001b[0m     sleep_generator,\n\u001b[0;32m    298\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    299\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    300\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[0;32m    157\u001b[0m         exc,\n\u001b[0;32m    158\u001b[0m         deadline,\n\u001b[0;32m    159\u001b[0m         sleep_iter,\n\u001b[0;32m    160\u001b[0m         error_list,\n\u001b[0;32m    161\u001b[0m         predicate,\n\u001b[0;32m    162\u001b[0m         on_error,\n\u001b[0;32m    163\u001b[0m         exception_factory,\n\u001b[0;32m    164\u001b[0m         timeout,\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# extract chunks which matches with your query\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m search_results \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the role of Lion in the story?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\in_memory.py:432\u001b[0m, in \u001b[0;36mInMemoryVectorStore.similarity_search_with_score\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m--> 432\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[0;32m    434\u001b[0m         embedding,\n\u001b[0;32m    435\u001b[0m         k,\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    437\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:283\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_query\u001b[1;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[0;32m    281\u001b[0m     result: EmbedContentResponse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39membed_content(request)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(result\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier\"\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerMinutePerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
     ]
    }
   ],
   "source": [
    "# extract chunks which matches with your query\n",
    "\n",
    "search_results = vector_store.similarity_search_with_score(\n",
    "    \"What is the theme of the book?\",\n",
    "    k = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566ba5f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m search_results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_results' is not defined"
     ]
    }
   ],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0ce6d",
   "metadata": {},
   "source": [
    "### What is RAG?\n",
    "Retrieve using semantic search and dump the similar chunks in the context of the prompt.\n",
    "LLM sees the question and retrieved docs in its prompt and generates tokens accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92552a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4152d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(doc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (doc,score) \u001b[38;5;129;01min\u001b[39;00m search_results)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc_content)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search_results' is not defined"
     ]
    }
   ],
   "source": [
    "doc_content = \"\\n\\n\".join(doc.page_content+\"\\n\"+\"=\"*50+\"\\n\" for (doc,score) in search_results)\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b1451",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:243\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[0;32m    244\u001b[0m         BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    245\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1437\u001b[0m, in \u001b[0;36mGenerativeServiceClient.batch_embed_contents\u001b[1;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1437\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m   1438\u001b[0m     request,\n\u001b[0;32m   1439\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m   1440\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1441\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m   1442\u001b[0m )\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    293\u001b[0m )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    295\u001b[0m     target,\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    297\u001b[0m     sleep_generator,\n\u001b[0;32m    298\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    299\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    300\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m _retry_error_helper(\n\u001b[0;32m    157\u001b[0m         exc,\n\u001b[0;32m    158\u001b[0m         deadline,\n\u001b[0;32m    159\u001b[0m         sleep_iter,\n\u001b[0;32m    160\u001b[0m         error_list,\n\u001b[0;32m    161\u001b[0m         predicate,\n\u001b[0;32m    162\u001b[0m         on_error,\n\u001b[0;32m    163\u001b[0m         exception_factory,\n\u001b[0;32m    164\u001b[0m         timeout,\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    210\u001b[0m         error_list,\n\u001b[0;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    212\u001b[0m         original_timeout,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m InMemoryVectorStore(embeddings)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Adding documents to vector store\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m document_ids \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mall_splits)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m init_chat_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\in_memory.py:195\u001b[0m, in \u001b[0;36mInMemoryVectorStore.add_documents\u001b[1;34m(self, documents, ids, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add documents to the store.\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 195\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39membed_documents(texts)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[0;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids must be the same length as texts. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ids and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m texts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:247\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    243\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[0;32m    244\u001b[0m             BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39membeddings])\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]"
     ]
    }
   ],
   "source": [
    "# make the LLM read see the prompt, and analyse the retrieved document, and generate response\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=\"AIzaSyDQU_npYx8c_ntmKXQKBkU6GBmHV-ndHRI\"\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "# (CLASSROOM DISCUSSION: What are vector stores? What do we make them?)\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Adding documents to vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama-3.3-70b-versatile\", model_provider=\"groq\")\n",
    "\n",
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(\n",
    "    context=doc_content,\n",
    "    question=\"What is the theme of the book?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d7fdba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is no mention of a lion in the provided context. The context appears to be a list of tale titles and snippets of text from various stories, but none of them mention a lion. I don't know the role of a lion in the story as it is not referenced in the given context."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better printing\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ceb2c",
   "metadata": {},
   "source": [
    "# RAG Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split given book into 1579 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# RAG summary\n",
    "\n",
    "# Read a doc\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"The Old Curiosity Shop.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split document into small chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split given book into {len(all_splits)} sub-documents.\")\n",
    "\n",
    "# embedding\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=\"AIzaSyCEe4LxbYaFR0qW3hHRk8l-097bt0CE7B0\"\n",
    ")\n",
    "\n",
    "# Create a vector store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Adding documents to vector store\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55042e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416  Indian  Tales \n",
      "\n",
      "\"Take  some  more  whiskey  and  go  on,\"  I  said. \n",
      "\"That  was  the  first  village  you  came  into.  How \n",
      "did  you  get  to  be  King  ?  \"\n",
      "==================================================\n",
      "\n",
      "\n",
      "Indian  Tales\n",
      "==================================================\n",
      "\n",
      "\n",
      "1 78  Indian  Tales\n",
      "==================================================\n",
      "\n",
      "\n",
      "238  Indian   Tales\n",
      "==================================================\n",
      "\n",
      "\n",
      "The  Man  Who  Would  be  King  427\n",
      "==================================================\n",
      "\n",
      "\n",
      "288  Indian    Tales\n",
      "==================================================\n",
      "\n",
      "\n",
      "226  Indian  Tales\n",
      "==================================================\n",
      "\n",
      "\n",
      "134  Indian  Tales \n",
      "\n",
      "' 'But  what  do  you  know  about  Polonius?\"  I \n",
      "demanded.  This  was  a  new  side  of  Mulvaney's \n",
      "character.\n",
      "==================================================\n",
      "\n",
      "\n",
      "The  Gate  of  the  Hundred  Sorrows  451\n",
      "==================================================\n",
      "\n",
      "\n",
      "In  the  House  of  Suddhoo  559\n",
      "==================================================\n",
      "\n",
      "\n",
      "The  Incarnation  of  Krishna  Mulvaney        473\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract chunks which matches with your query\n",
    "\n",
    "search_results = vector_store.similarity_search_with_score(\n",
    "    \"What is the theme of the book?\",\n",
    "    k = 11\n",
    ")\n",
    "\n",
    "doc_content = \"\\n\\n\".join(doc.page_content+\"\\n\"+\"=\"*50+\"\\n\" for (doc,score) in search_results)\n",
    "print(doc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94122ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't know the role of the Lion in the story as there is no mention of a Lion in the provided context. The context appears to be a collection of Indian Tales with various titles and snippets of text, but none of them mention a Lion. I couldn't find any relevant information to answer the question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\"\"\"\n",
    "\n",
    "response = model.invoke(prompt_template.format(\n",
    "    context=doc_content,\n",
    "    question=\"What is the theme of the book?\"))\n",
    "\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
